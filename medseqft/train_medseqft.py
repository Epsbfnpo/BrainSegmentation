import argparse
import sys
import os
import torch
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP
from pathlib import Path
from torch.optim import AdamW
from torch.optim.lr_scheduler import CosineAnnealingLR
from torch.cuda.amp import GradScaler, autocast
from tensorboardX import SummaryWriter

# Import local components
from components import get_dataloaders, MedSeqFTWrapper, MedSeqFTLoss
from utils_medseqft import SignalHandler, check_slurm_deadline, robust_one_hot

# --- MONAI Imports ---
from monai.metrics import DiceMetric
from monai.inferers import sliding_window_inference


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--split_json", required=True)
    parser.add_argument("--buffer_json", default=None)
    parser.add_argument("--results_dir", required=True)
    parser.add_argument("--pretrained_checkpoint", required=True)
    parser.add_argument("--volume_stats", type=str, default=None)

    # Model params
    parser.add_argument("--roi_x", default=128, type=int)
    parser.add_argument("--roi_y", default=128, type=int)
    parser.add_argument("--roi_z", default=128, type=int)
    parser.add_argument("--in_channels", default=1, type=int)
    parser.add_argument("--out_channels", default=87, type=int)
    parser.add_argument("--feature_size", default=48, type=int)

    # Training params
    parser.add_argument("--epochs", default=2000, type=int)
    parser.add_argument("--batch_size", default=2, type=int)
    parser.add_argument("--lr", default=5e-5, type=float)
    parser.add_argument("--weight_decay", default=1e-5, type=float)
    parser.add_argument("--num_workers", default=4, type=int)
    parser.add_argument("--cache_rate", default=0.0, type=float)
    parser.add_argument("--lambda_kd", default=1.0, type=float)
    parser.add_argument("--grad_accum_steps", default=2, type=int)

    # Flags
    parser.add_argument("--apply_spacing", action="store_true", default=True)
    parser.add_argument("--target_spacing", nargs=3, type=float, default=[0.8, 0.8, 0.8])
    parser.add_argument("--apply_orientation", action="store_true", default=True)
    parser.add_argument("--foreground_only", action="store_true", default=True)

    args = parser.parse_args()

    # --- 1. DDP åˆå§‹åŒ– (å…³é”®ä¿®æ”¹) ---
    # torchrun ä¼šè‡ªåŠ¨è®¾ç½® LOCAL_RANK ç­‰ç¯å¢ƒå˜é‡
    if "LOCAL_RANK" in os.environ:
        dist.init_process_group(backend="nccl")
        local_rank = int(os.environ["LOCAL_RANK"])
        global_rank = int(os.environ["RANK"])
        torch.cuda.set_device(local_rank)
        device = torch.device("cuda", local_rank)
        is_main_process = (global_rank == 0)
    else:
        # å…¼å®¹é DDP è¿è¡Œ
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        is_main_process = True
        local_rank = 0

    res_dir = Path(args.results_dir)
    if is_main_process:
        res_dir.mkdir(parents=True, exist_ok=True)

    # --- 2. ä»…åœ¨ Rank 0 åˆå§‹åŒ– Writer ---
    if is_main_process:
        writer = SummaryWriter(log_dir=str(res_dir / "logs"))
    else:
        writer = None

    sig_handler = SignalHandler()

    # Data
    # æ³¨æ„ï¼šåœ¨ç†æƒ³ DDP ä¸­è¿™é‡Œæœ€å¥½ç”¨ DistributedSamplerï¼Œä½†ç›®å‰éšæœº shuffle ä¹Ÿèƒ½è·‘ï¼Œ
    # ç›¸å½“äºå˜ç›¸å¢å¤§äº† Batch Size (4å¡ x BS2 = æœ‰æ•ˆBS 8)
    train_loader, val_loader = get_dataloaders(args)

    # Models
    if is_main_process:
        print("ğŸ—ï¸ Building Student Model...")
    student = MedSeqFTWrapper(args, device).to(device)
    student.load_pretrained(args.pretrained_checkpoint)

    # --- 3. DDP åŒ…è£…æ¨¡å‹ ---
    if dist.is_initialized():
        student = DDP(student, device_ids=[local_rank], output_device=local_rank)

    if is_main_process:
        print("ğŸ—ï¸ Building Teacher Model (Frozen Source)...")
    teacher = MedSeqFTWrapper(args, device).to(device)
    teacher.load_pretrained(args.pretrained_checkpoint)
    teacher.eval()
    for p in teacher.parameters(): p.requires_grad = False

    # Optimization
    optimizer = AdamW(student.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scheduler = CosineAnnealingLR(optimizer, T_max=args.epochs)
    scaler = GradScaler()
    loss_fn = MedSeqFTLoss(num_classes=args.out_channels, lambda_kd=args.lambda_kd)

    dice_metric = DiceMetric(include_background=True, reduction="mean", get_not_nans=False)

    # Resume Logic
    start_epoch = 0
    best_dice = 0.0
    ckpt_path = res_dir / "latest_checkpoint.pt"

    # ä»… Rank 0 è¯»å–å’Œå¹¿æ’­ Checkpoint (æˆ–è€…è®©å¤§å®¶éƒ½å»è¯»)
    # ç®€å•èµ·è§ï¼Œè¿™é‡Œå¤§å®¶éƒ½å°è¯•è¯»ï¼Œåªè¦æ–‡ä»¶å­˜åœ¨ä¸å†²çªå³å¯ (è¯»æ“ä½œæ˜¯å®‰å…¨çš„)
    if ckpt_path.exists():
        if is_main_process:
            print(f"ğŸ”„ Resuming from {ckpt_path}")
        # map_location å¿…é¡»æŒ‡å®šåˆ°å½“å‰ GPU
        ckpt = torch.load(ckpt_path, map_location=device)

        # å¤„ç† DDP å¸¦æ¥çš„ module. å‰ç¼€é—®é¢˜ (å¦‚æœä¹‹å‰ä¿å­˜çš„æ˜¯ DDP state dict)
        # ä½ çš„ MedSeqFTWrapper.load_pretrained å·²ç»å¤„ç†äº† module. å‰ç¼€ï¼Œä½†è¿™é‡Œæ˜¯ç›´æ¥ load_state_dict
        # æˆ‘ä»¬éœ€è¦ç®€å•æ¸…æ´—ä¸€ä¸‹
        model_state = ckpt['model']
        # å¦‚æœå½“å‰æ˜¯ DDPï¼Œä½† checkpoint ä¸æ˜¯ (æˆ–è€…åä¹‹)ï¼Œkey ä¼šå¯¹ä¸ä¸Š
        # æœ€ç¨³å¦¥çš„æ–¹å¼ï¼šç›´æ¥åŠ è½½ã€‚å› ä¸ºä¿å­˜æ—¶é€šå¸¸å»ºè®® student.module.state_dict()
        # è¿™é‡Œå‡è®¾ä¿å­˜çš„æ˜¯ student.state_dict() (å³åŒ…å«äº† module. å‰ç¼€)
        try:
            student.load_state_dict(model_state)
        except RuntimeError:
            # å°è¯•å»æ‰ module. å‰ç¼€ (å¦‚æœ ckpt æœ‰è€Œ model æ²¡æœ‰)
            new_state = {k.replace('module.', ''): v for k, v in model_state.items()}
            # å¦‚æœ model æœ‰ module. (DDP) è€Œ ckpt æ²¡æœ‰ï¼ŒåŠ ä¸Š
            if isinstance(student, DDP):
                # è¿™ç§æƒ…å†µä¸‹é€šå¸¸ç›´æ¥ load å°±è¡Œï¼Œå› ä¸º DDP åŒ…è£…å key å˜äº†
                # è¿™é‡Œåšä¸ªç®€å•çš„ fallback
                student.module.load_state_dict(new_state)
            else:
                student.load_state_dict(new_state)

        optimizer.load_state_dict(ckpt['optimizer'])
        scheduler.load_state_dict(ckpt['scheduler'])
        start_epoch = ckpt['epoch'] + 1
        best_dice = ckpt.get('best_dice', 0.0)
        if is_main_process:
            print(f"   Last Best Dice: {best_dice:.4f}")

    if is_main_process:
        print(f"ğŸš€ Starting MedSeqFT Training (KD-based FFT) for {args.epochs} epochs")

    optimizer.zero_grad()

    for epoch in range(start_epoch, args.epochs):
        # å¦‚æœä½¿ç”¨äº† DistributedSamplerï¼Œè¿™é‡Œéœ€è¦ train_loader.sampler.set_epoch(epoch)

        student.train()
        epoch_loss = 0
        epoch_seg = 0
        epoch_kd = 0
        steps = 0

        for batch in train_loader:
            if sig_handler.stop_requested or check_slurm_deadline(buffer_seconds=600):
                if is_main_process:
                    print(f"ğŸ›‘ æ£€æµ‹åˆ°é€€å‡ºä¿¡å·æˆ–è¶…æ—¶ (Epoch {epoch})ï¼Œä¿å­˜æ–­ç‚¹å¹¶é€€å‡º...")
                    state = {
                        'model': student.state_dict(),
                        'optimizer': optimizer.state_dict(),
                        'scheduler': scheduler.state_dict(),
                        'epoch': epoch - 1,
                        'best_dice': best_dice
                    }
                    torch.save(state, ckpt_path)
                    print("ğŸ‘‹ ä¼˜é›…é€€å‡º (Exit 0)")
                # ç¡®ä¿æ‰€æœ‰è¿›ç¨‹åŒæ­¥é€€å‡º
                if dist.is_initialized():
                    dist.barrier()
                sys.exit(0)

            if not batch:
                if is_main_process:
                    print(f"âš ï¸ Warning: Skipped empty batch at epoch {epoch}")
                continue
                # ==============================

            img = batch["image"].to(device)
            label = batch["label"].to(device)

            with autocast():
                pred = student(img)
                with torch.no_grad():
                    teacher_pred = teacher(img)

                if teacher_pred.shape[1] != pred.shape[1]:
                    min_ch = min(teacher_pred.shape[1], pred.shape[1])
                    teacher_pred_safe = teacher_pred[:, :min_ch, ...]
                    pred_safe_for_kd = pred[:, :min_ch, ...]
                    total_loss, l_seg, l_kd = loss_fn(pred, label, teacher_pred_safe, pred_kd=pred_safe_for_kd)
                else:
                    total_loss, l_seg, l_kd = loss_fn(pred, label, teacher_pred)

                loss = total_loss / args.grad_accum_steps

            scaler.scale(loss).backward()

            if (steps + 1) % args.grad_accum_steps == 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(student.parameters(), 12.0)
                scaler.step(optimizer)
                scaler.update()
                optimizer.zero_grad()

            # ç®€å•çš„ Loss èšåˆç”¨äºæ‰“å° (åªåœ¨ Rank 0 æ‰“å°)
            if dist.is_initialized():
                dist.all_reduce(total_loss)
                total_loss /= dist.get_world_size()  # å¹³å‡ Loss

            epoch_loss += total_loss.item()
            epoch_seg += l_seg.item()
            epoch_kd += l_kd.item()
            steps += 1

        if steps % args.grad_accum_steps != 0:
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(student.parameters(), 12.0)
            scaler.step(optimizer)
            scaler.update()
            optimizer.zero_grad()

        scheduler.step()

        # --- VALIDATION LOOP ---
        if (epoch + 1) % 5 == 0:
            student.eval()
            if is_main_process:
                print(f"ğŸ” Validating at epoch {epoch}...")

            # åˆ›å»ºä¸€ä¸ªç”¨äº Metric èšåˆçš„ List
            val_dice_list = []

            with torch.no_grad():
                for val_batch in val_loader:
                    # éªŒè¯é˜¶æ®µçš„ä¿¡å·æ£€æŸ¥
                    if sig_handler.stop_requested or check_slurm_deadline(buffer_seconds=600):
                        if is_main_process:
                            print(f"ğŸ›‘ éªŒè¯é˜¶æ®µé€€å‡º (Epoch {epoch})...")
                            state = {
                                'model': student.state_dict(),
                                'optimizer': optimizer.state_dict(),
                                'scheduler': scheduler.state_dict(),
                                'epoch': epoch,
                                'best_dice': best_dice
                            }
                            torch.save(state, ckpt_path)
                        if dist.is_initialized():
                            dist.barrier()
                        sys.exit(0)

                    v_img = val_batch["image"].to(device)
                    v_label = val_batch["label"].to(device)

                    with autocast():
                        val_out = sliding_window_inference(
                            v_img, (args.roi_x, args.roi_y, args.roi_z), 4, student, overlap=0.5
                        )

                    val_pred = torch.argmax(val_out, dim=1, keepdim=True)
                    v_label_expanded, brain_mask = robust_one_hot(
                        v_label, num_classes=args.out_channels, ignore_index=-1
                    )
                    val_pred_expanded, _ = robust_one_hot(
                        val_pred, num_classes=args.out_channels, ignore_index=-1
                    )
                    val_pred_expanded = val_pred_expanded * brain_mask

                    # è®¡ç®—å½“å‰ Batch çš„ Dice
                    dice_metric(y_pred=val_pred_expanded, y=v_label_expanded)

                # èšåˆæœ¬åœ° Dice
                local_dice = dice_metric.aggregate().item()
                dice_metric.reset()

                # DDP: èšåˆæ‰€æœ‰å¡çš„ Dice
                if dist.is_initialized():
                    metric_tensor = torch.tensor(local_dice).to(device)
                    dist.all_reduce(metric_tensor)
                    mean_dice = metric_tensor.item() / dist.get_world_size()
                else:
                    mean_dice = local_dice

                if is_main_process:
                    print(f"Epoch {epoch}: Train Loss={epoch_loss:.4f} | Val Dice={mean_dice:.4f}")
                    if writer:
                        writer.add_scalar("val/dice", mean_dice, epoch)

                    if mean_dice > best_dice:
                        best_dice = mean_dice
                        print(f"ğŸŒŸ New Best Dice: {best_dice:.4f}")
                        # ä¿å­˜æœ€ä½³æ¨¡å‹ (æ³¨æ„: DDPæ—¶ student.state_dict() åŒ…å« module.)
                        torch.save(student.state_dict(), res_dir / "best_model.pt")
                        state = {
                            'model': student.state_dict(),
                            'optimizer': optimizer.state_dict(),
                            'scheduler': scheduler.state_dict(),
                            'epoch': epoch,
                            'best_dice': best_dice
                        }
                        torch.save(state, ckpt_path)

        # ä»…åœ¨ Rank 0 è®°å½•è®­ç»ƒæ—¥å¿—å’Œä¿å­˜ Checkpoint
        if is_main_process:
            if writer:
                writer.add_scalar("train/loss", epoch_loss, epoch)
                writer.add_scalar("train/seg_loss", epoch_seg, epoch)
                writer.add_scalar("train/kd_loss", epoch_kd, epoch)
                writer.add_scalar("train/lr", optimizer.param_groups[0]['lr'], epoch)

            state = {
                'model': student.state_dict(),
                'optimizer': optimizer.state_dict(),
                'scheduler': scheduler.state_dict(),
                'epoch': epoch,
                'best_dice': best_dice
            }
            torch.save(state, ckpt_path)

            if epoch == args.epochs - 1:
                torch.save(student.state_dict(), res_dir / "final_model.pt")

    if is_main_process:
        print("ğŸï¸ Training Finished.")

    # é”€æ¯è¿›ç¨‹ç»„
    if dist.is_initialized():
        dist.destroy_process_group()


if __name__ == "__main__":
    main()